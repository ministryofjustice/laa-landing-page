name: RDS Snapshot (Reusable)

on:
  workflow_call:
    inputs:
      environment:
        description: 'Environment to create snapshot for'
        required: true
        type: string
      snapshot_suffix:
        description: 'Optional suffix for snapshot name'
        required: false
        type: string
        default: ''
      purpose:
        description: 'Purpose of the snapshot (manual-backup, pre-deployment, etc.)'
        required: true
        type: string
    secrets:
      KUBE_NAMESPACE:
        required: true
      KUBE_CLUSTER:
        required: true
      KUBE_CERT:
        required: true
      KUBE_TOKEN:
        required: true
      ECR_ROLE_TO_ASSUME:
        required: true
    outputs:
      snapshot_id:
        description: 'The created snapshot identifier'
        value: ${{ jobs.create-snapshot.outputs.snapshot_id }}

jobs:
  create-snapshot:
    runs-on: ubuntu-latest
    outputs:
      snapshot_id: ${{ steps.create-snapshot.outputs.snapshot_id }}
    steps:
      - uses: actions/checkout@v5.0.0

      # Assume role in Cloud Platform
      - uses: aws-actions/configure-aws-credentials@v4.3.1
        with:
          role-to-assume: ${{ secrets.ECR_ROLE_TO_ASSUME }}
          aws-region: eu-west-2

      - name: Configure kubectl
        run: |
          echo "${{ secrets.KUBE_CERT }}" > ca.crt
          kubectl config set-cluster ${KUBE_CLUSTER} --certificate-authority=./ca.crt --server=https://${KUBE_CLUSTER}
          kubectl config set-credentials deploy-user --token=${{ secrets.KUBE_TOKEN }}
          kubectl config set-context ${KUBE_CLUSTER} --cluster=${KUBE_CLUSTER} --user=deploy-user --namespace=${KUBE_NAMESPACE}
          kubectl config use-context ${KUBE_CLUSTER}
        env:
          KUBE_NAMESPACE: ${{ secrets.KUBE_NAMESPACE }}
          KUBE_CLUSTER: ${{ secrets.KUBE_CLUSTER }}

      - name: Create RDS Snapshot
        id: create-snapshot
        run: |
          #!/bin/bash
          set -e  # Exit on any error
          
          TIMESTAMP=$(date +"%Y%m%d-%H%M%S")
          ENV_NAME="${{ inputs.environment }}"
          SNAPSHOT_SUFFIX="${{ inputs.snapshot_suffix }}"
          PURPOSE="${{ inputs.purpose }}"
          NAMESPACE="${{ secrets.KUBE_NAMESPACE }}"
          
          # Automatically construct service pod label based on environment
          SERVICE_POD_LABEL="irsa-laa-landing-page-${ENV_NAME}-service-pod"
          
          echo "=== RDS Snapshot Creation Started ==="
          echo "Environment: ${ENV_NAME}"
          echo "Purpose: ${PURPOSE}"
          
          # Validate required parameters
          if [ -z "$ENV_NAME" ] || [ -z "$NAMESPACE" ]; then
            echo "ERROR: Missing required parameters"
            exit 1
          fi
          
          # Get DB instance identifier with error handling
          echo "Retrieving database configuration..."
          if ! RDS_DB_IDENTIFIER=$(kubectl get secret rds-postgresql-instance-output -n "${NAMESPACE}" -o jsonpath='{.data.rds_instance_endpoint}' 2>/dev/null | base64 -d | cut -d. -f1 | xargs printf "%s"); then
            echo "ERROR: Failed to retrieve database configuration from Kubernetes secret"
            exit 1
          fi
          
          if [ -z "$RDS_DB_IDENTIFIER" ]; then
            echo "ERROR: Database configuration is invalid"
            exit 1
          fi
          
          # Create snapshot identifier with SILAS application name
          if [ -n "$SNAPSHOT_SUFFIX" ]; then
            SNAPSHOT_IDENTIFIER="SILAS-${ENV_NAME}-${TIMESTAMP}-${SNAPSHOT_SUFFIX}"
          else
            SNAPSHOT_IDENTIFIER="SILAS-${ENV_NAME}-${TIMESTAMP}"
          fi
          
          echo "Creating snapshot: ${SNAPSHOT_IDENTIFIER}"
          
          # Scale up the service pod deployment if needed
          echo "Preparing service pod..."
          if ! CURRENT_REPLICAS=$(kubectl get deployment "${SERVICE_POD_LABEL}" -n "${NAMESPACE}" -o jsonpath='{.spec.replicas}' 2>/dev/null); then
            echo "ERROR: Service pod deployment not found"
            exit 1
          fi
          
          if [ "$CURRENT_REPLICAS" = "0" ] || [ -z "$CURRENT_REPLICAS" ]; then
            echo "Scaling up service pod..."
            if ! kubectl scale deployment "${SERVICE_POD_LABEL}" --replicas=1 -n "${NAMESPACE}"; then
              echo "ERROR: Failed to scale up service pod"
              exit 1
            fi
            
            echo "Waiting for service pod to be ready..."
            if ! kubectl wait --for=condition=Available deployment/"${SERVICE_POD_LABEL}" -n "${NAMESPACE}" --timeout=120s; then
              echo "ERROR: Service pod failed to become available within 120 seconds"
              exit 1
            fi
          else
            echo "Service pod is already running"
          fi
          
          # Get the service pod name with error handling
          if ! SERVICE_POD_NAME=$(kubectl get pods -n "${NAMESPACE}" -l name="${SERVICE_POD_LABEL}" -o jsonpath='{.items[0].metadata.name}' 2>/dev/null); then
            echo "ERROR: Service pod not found"
            exit 1
          fi
          
          if [ -z "$SERVICE_POD_NAME" ]; then
            echo "ERROR: Service pod not available"
            exit 1
          fi
          
          # Create the snapshot using the service pod
          echo "Creating RDS snapshot..."
          kubectl exec -n "${NAMESPACE}" "${SERVICE_POD_NAME}" -- bash -c "
          set -e
          
          echo 'Starting RDS snapshot creation...'
          echo 'Application: SILAS'
          echo 'Environment: ${ENV_NAME}'
          
          # Create the snapshot
          aws rds create-db-snapshot \
            --db-instance-identifier '${RDS_DB_IDENTIFIER}' \
            --db-snapshot-identifier '${SNAPSHOT_IDENTIFIER}' \
            --tags Key=Application,Value='SILAS' Key=Environment,Value='${ENV_NAME}' Key=CreatedBy,Value='github-actions' Key=Purpose,Value='${PURPOSE}'
          
          echo 'Snapshot creation initiated successfully!'
          
          # Wait for snapshot to be available
          echo 'Waiting for snapshot to become available...'
          aws rds wait db-snapshot-completed \
            --db-snapshot-identifier '${SNAPSHOT_IDENTIFIER}' \
            --cli-read-timeout 0 \
            --cli-connect-timeout 60
          
          echo 'Snapshot is now available!'
          "
          
          echo "Snapshot creation completed successfully!"
          
          # Validate snapshot was created successfully
          echo "Validating snapshot..."
          if ! kubectl exec -n "${NAMESPACE}" "${SERVICE_POD_NAME}" -- aws rds describe-db-snapshots --db-snapshot-identifier "${SNAPSHOT_IDENTIFIER}" --query 'DBSnapshots[0].Status' --output text > /dev/null 2>&1; then
            echo "ERROR: Failed to validate snapshot"
            exit 1
          fi
          
          SNAPSHOT_STATUS=$(kubectl exec -n "${NAMESPACE}" "${SERVICE_POD_NAME}" -- aws rds describe-db-snapshots --db-snapshot-identifier "${SNAPSHOT_IDENTIFIER}" --query 'DBSnapshots[0].Status' --output text)
          
          if [ "$SNAPSHOT_STATUS" != "available" ]; then
            echo "WARNING: Snapshot is not yet available (Status: ${SNAPSHOT_STATUS})"
            echo "This is normal for large databases - snapshot will become available shortly"
          else
            echo "âœ… Snapshot is available and ready to use"
          fi
          
          # Scale down the service pod deployment if it was scaled up
          echo "Cleaning up service pod..."
          if [ "$CURRENT_REPLICAS" = "0" ] || [ -z "$CURRENT_REPLICAS" ]; then
            echo "Scaling down service pod..."
            if ! kubectl scale deployment "${SERVICE_POD_LABEL}" --replicas=0 -n "${NAMESPACE}"; then
              echo "WARNING: Failed to scale down service pod"
              echo "You may need to manually scale it down later"
            else
              echo "Service pod scaled down successfully"
            fi
          else
            echo "Service pod was already running, leaving it as is"
          fi
          
          echo "=== RDS Snapshot Creation Completed Successfully ==="
          
          # Set output for other jobs
          echo "snapshot_id=${SNAPSHOT_IDENTIFIER}" >> $GITHUB_OUTPUT
        env:
          KUBE_NAMESPACE: ${{ secrets.KUBE_NAMESPACE }}
